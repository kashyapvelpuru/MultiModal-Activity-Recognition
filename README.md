# MultiModal-Activity-Recognition

Human visual behaviour is a very rich source of information to understand human activities. The
combination of this visual behaviour with scene information could significantly boost the performance
of recognizing activities. Previous works have focused on using both supervised and unsupervised
methods for determination of predefined activities only from short term and long term visual behaviour.
In my work, I integrate two different modalities namely visual gaze behaviour and visual scene
information modals using late fusion technique, provide a comparison on three different deep learning
architectures with a combination of CNN and LSTM for activity recognition by using long term gaze
information and egocentric view based visual scene information.
